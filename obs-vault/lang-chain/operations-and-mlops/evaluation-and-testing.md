---
tags:
  - langchain
  - operations
  - mloops
  - evaluation
  - testing
  - ragas
---
# 運用：評価とテスト

LLMアプリケーション、特にRAGやエージェントシステムは、その非決定的な性質上、従来のソフトウェアのような単純な単体テストや統合テストだけでは品質を保証できません。「正解」が一つではないため、「期待される出力と一致するか」を検証することが困難だからです。

そのため、LLMシステムの品質保証には、**継続的な評価（Continuous Evaluation）**という考え方が不可欠になります。これは、アプリケーションの性能を定量的な指標で測定し、変更が加わるたびにその性能が劣化（リグレッション）していないかを確認するプロセスです。

## 1. なぜ評価が重要なのか？

*   **リグレッションの防止**: プロンプトの変更、コンポーネントの更新、基盤モデルのバージョンアップなど、システムへのあらゆる変更が、予期せぬ性能低下を引き起こす可能性があります。定期的な評価は、これらのリグレッションを早期に検知するために不可欠です。
*   **客観的な改善**: 「プロンプトを少し変更したら、応答が良くなった気がする」といった主観的な判断から脱却し、複数の評価指標に基づいて改善のインパクトを客観的に測定できます。
*   **コンポーネントの最適化**: RAGシステムにおけるチャンキング戦略、埋め込みモデル、リトリーバーの`k`値など、様々なハイパーパラメータを調整する際に、どの組み合わせが最も良い性能を発揮するかをデータに基づいて判断できます。

## 2. RAGシステムの評価指標

RAGシステムの評価は、主に「取得（Retrieval）」と「生成（Generation）」の2つの側面から行われます。**Ragas**のような評価フレームワークは、これらの側面を測定するための標準的な指標を提供しています。

### 2.1. 取得（Retrieval）の評価

リトリーバーが、生成に必要な正しいコンテキスト（文脈）を取得できているかを評価します。

*   **Context Precision**: 取得されたコンテキストのうち、どれだけが実際に質問と関連しているか。ノイズの多い、無関係な情報を取得していないかを測定します。
*   **Context Recall**: 質問に答えるために必要な全ての情報が、取得されたコンテキストの中に含まれているか。情報の取得漏れがないかを測定します。

### 2.2. 生成（Generation）の評価

取得されたコンテキストを基に、LLMがどれだけ高品質な回答を生成できたかを評価します。

*   **Faithfulness (忠実度)**: 生成された回答が、提供されたコンテキストのみに忠実に基づいており、外部知識や幻覚（ハルシネーション）を含んでいないか。
*   **Answer Relevancy (回答の関連性)**: 生成された回答が、ユーザーの質問に直接的かつ適切に答えているか。質問から逸脱した回答をしていないかを測定します。

## 3. 評価データセットの構築

効果的な評価を行うためには、質の高い評価データセット（質問と、理想的な回答や参照すべきコンテキストのペア）が必要です。

*   **手動作成**: 人間の専門家が、実際のユースケースを想定した質の高いQ&Aペアを作成します。最も高品質ですが、コストがかかります。
*   **既存データからの生成**: FAQリストやドキュメントの要約など、既存のデータ資産からQ&Aペアを生成します。
*   **LLMによる合成データ生成**: LLMを使って、ドキュメントから評価用のQ&Aペアを自動生成させるアプローチです。LangChainは、これを行うための`QAGenerationChain`などを提供しており、低コストで大規模な評価セットを構築するのに役立ちます。

## 4. MLOps：CI/CDへの評価の統合

LLMアプリケーションを本番環境で継続的に改善していくためには、評価プロセスを自動化し、CI/CDパイプラインに組み込む**MLOps（Machine Learning Operations）**のアプローチが不可欠です。

RAGシステムにおけるMLOpsの評価サイクルは、以下のように構築できます。

1.  **コードの変更**: 開発者がプロンプトの修正やライブラリの更新などをGitリポジトリにプッシュします。
2.  **CIトリガー**: GitHub ActionsなどのCIツールが変更を検知し、自動的にテストと評価のワークフローを開始します。
3.  **評価の実行**:
    *   インジェストパイプラインを再実行し、ベクトルストアを更新します（必要な場合）。
    *   事前に準備された評価データセットを使って、RAGアプリケーションに一連の質問を投げかけます。
    *   Ragasなどの評価ライブラリを使い、生成された回答のFaithfulnessやContext Precisionなどのスコアを計算します。
4.  **結果のレポーティング**:
    *   計算された評価スコアを、前のバージョン（mainブランチなど）のスコアと比較します。
    *   結果をPull Requestのコメントとして投稿したり、専用のダッシュボードに表示したりします。
5.  **意思決定**: 開発者やレビュー担当者は、評価レポートを見て、性能が向上したか、あるいは許容できないレベルまで低下したかを確認し、変更をマージするかどうかの判断を下します。

このサイクルを確立することで、LLMアプリケーションへの全ての変更が、システムの品質に与える影響を客観的に監視できるようになり、自信を持って迅速な改善を続けることが可能になります。
