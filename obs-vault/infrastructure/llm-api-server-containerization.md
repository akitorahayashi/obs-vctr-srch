---
tags:
  - infrastructure
  - docker
  - llm
  - api
  - ollama
---
# DockerによるLLM APIサーバーのコンテナ化

## 1. はじめに：なぜLLMをコンテナ化するのか？

大規模言語モデル（LLM）をローカル環境で手軽に実行できるOllamaは、開発や実験の初期段階において非常に強力なツールです。しかし、開発したアプリケーションをチームメンバーと共有したり、ステージング環境や本番環境へデプロイしたりする段階になると、新たな課題が浮上します。

### 1.1 「私の環境では動く」問題

あるエンジニアが、自身のローカルマシンでOllamaを使い、画期的なアプリケーションを開発したとします。すべてが完璧に動作するため、彼は自信を持ってソースコードを同僚に共有します。しかし、同僚の環境では「モデルが見つからない」「バージョンが違う」「なぜか挙動が異なる」といった問題が発生し、アプリケーションは正しく実行できません。

これは「私の環境では動く（It works on my machine）」問題として知られる典型的なシナリオです。特にAI/ML開発では、この問題がより深刻になります。アプリケーションコードだけでなく、使用するLLMのモデルファイル（数ギガバイトに及ぶことも珍しくありません）、モデルのバージョン、さらには特定のライブラリやシステム依存関係など、環境を構成する要素が複雑かつ巨大であるためです。これらの要素のわずかな違いが、アプリケーションの動作に予期せぬ影響を与えてしまうのです。

### 1.2 解決策としてのDocker

この環境再現性の課題を解決する強力なソリューションが[[introduction-to-docker]]です。Dockerは、アプリケーションをその依存関係とともに「コンテナ」と呼ばれる隔離された環境にパッケージ化する技術です。LLMアプリケーション開発において、Dockerは以下の重要な利点をもたらします。

* **再現性 (Reproducibility):** Dockerfileというテキストファイルに環境の構成情報をすべて記述します。このファイルは、誰がどこで実行しても、寸分違わず同じ環境を構築するための「設計図」として機能します。これにより、「私の環境では動く」問題は根本的に解決されます。
* **可搬性 (Portability):** Dockerは、アプリケーション、依存ライブラリ、ランタイム、そしてLLMモデル自体をも一つの「イメージ」としてパッケージ化します。このイメージは、開発者のノートPCからテストサーバー、本番のクラウド環境に至るまで、あらゆる場所で一貫した動作を保証します。
* **依存関係の隔離 (Dependency Isolation):** コンテナは、ホストマシンや他のコンテナから隔離された独自のファイルシステムとプロセス空間を持ちます。これにより、特定のPythonバージョンやCUDAツールキットなど、プロジェクト固有の依存関係が他のソフトウェアと衝突することを防ぎます。

単なるデプロイツールとしてだけでなく、AIアプリケーションのスタック全体（コード、依存関係、そしてモデル）を一つのバージョン管理可能な不変の成果物として扱うことで、ソフトウェア工学のベストプラクティスをLLM開発に適用することが可能になります。これは、アプリケーションを本番環境で安定して運用するための、きわめて重要な一歩です。

### 1.3 この章で学ぶこと

この章では、ローカルでの実験段階を卒業し、LLMアプリケーションを堅牢で再利用可能なコンポーネントへと昇華させるための具体的な手順を学びます。まずは標準的なOllamaのコンテナを起動し、そのAPIと対話する方法を習得します。次に、この章の核心であるDockerfileを用いて、特定のLLMモデルを内包したカスタムイメージを構築します。最終的な目標は、どこにでも持ち運んで実行できる、自己完結したLLM APIサーバーを構築できるようになることです。

## 2. 基本のOllamaサーバーを起動する

最初のステップとして、Dockerを使ってOllamaの公式イメージからAPIサーバーを起動します。この方法は、主にローカルでの開発や、モデルを手軽に試す際に非常に便利です。

### 2.1 公式イメージの取得

Ollamaは、Docker Hub上で公式イメージ ollama/ollama を提供しています。まずはこのイメージをローカルにダウンロードします。

イメージには「タグ」が付与されており、バージョンを指定できます。:latest は最新版を指しますが、本番環境などでは :0.1.44 のようにバージョンを明示的に指定することで、予期せぬアップデートによる影響を防ぎ、環境の安定性を高めることが推奨されます。GPUを利用する場合は:rocm のようなGPU対応タグも存在します。

ターミナルで以下のコマンドを実行し、最新の公式イメージを取得しましょう。

```bash
docker pull ollama/ollama:latest
```

### 2.2 コンテナの起動：コマンドの徹底解説

次に、取得したイメージからコンテナを起動します。以下のコマンドは、Ollamaサーバーを永続的に実行するための標準的な方法です。

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

このコマンドは多くのオプション（フラグ）を含んでいますが、それぞれがLLMサーバーを運用する上で重要な意味を持っています。

**表2.1: docker run オプションの解説**

| フラグ | 説明 | LLMにおける重要性 |
| :---- | :---- | :---- |
| -d | デタッチモード。コンテナをバックグラウンドで実行します。 | このフラグにより、ターミナルを占有することなくLLMサーバーを起動し続けられます。 |
| -p 11434:11434 | ポートマッピング。ホストのポート11434をコンテナのポート11434に接続します。 | OllamaのREST APIがデフォルトで待ち受けるポートです。これを外部に公開することで、APIリクエストを送信できるようになります。 |
| -v ollama:/root/.ollama | ボリュームマウント。ollamaという名前付きボリュームを、コンテナ内の/root/.ollamaディレクトリにマウントします。 | LLMのモデルファイルは巨大です。この設定により、ダウンロードしたモデルがホストマシン上のボリュームに保存され、コンテナを削除・再作成してもモデルが失われるのを防ぎます。/root/.ollamaはOllamaのデフォルトのモデル保存場所です。 |
| --name ollama | コンテナにollamaという分かりやすい名前を付けます。 | 長いコンテナIDの代わりに、docker stop ollamaのように名前でコンテナを簡単に管理できるようになります。 |
| --gpus all (任意) | ホストマシン上の利用可能な全てのGPUへのアクセスをコンテナに許可します。 | モデルの推論を大幅に高速化するために不可欠です。利用するには、ホストマシンにNVIDIA Container Toolkitがインストールされている必要があります。 |

このコマンドで重要なのは `-v` フラグです。これは、コンテナの状態（ダウンロードしたモデル）をコンテナの外部、つまりホストマシンに依存させる設計です。ローカル開発ではコンテナ自体を軽量に保ち、モデルの永続化ができるため非常に便利ですが、このコンテナを別のマシンに移動させてもモデルはついてきません。この「外部依存」という特性が、後のセクションで学ぶ「モデルの焼き込み」アプローチの重要性を理解する鍵となります。

### 2.3 サーバーの動作確認

コンテナが正しく起動したかを確認しましょう。いくつかのステップで検証します。

1. コンテナの実行状態を確認:
   `docker ps` コマンドを実行します。ollama という名前のコンテナがUpの状態でリストに表示されていれば成功です。
   ```bash
   docker ps
   ```

2. サーバーログを確認:
   もしコンテナが起動しない、または何か問題が疑われる場合は、ログを確認するのが第一歩です。
   ```bash
   docker logs ollama
   ```

3. APIサーバーのヘルスチェック:
   `curl` コマンドを使って、APIサーバーがリクエストを受け付けられる状態かを確認します。
   ```bash
   curl http://localhost:11434
   ```

   ターミナルに `Ollama is running` と表示されれば、APIサーバーは正常に起動し、外部からのアクセスを待っている状態です。

## 3. APIサーバーとの対話

サーバーが起動したところで、次はこのAPIサーバーと対話する方法を学びます。具体的には、モデルをダウンロードし、`curl` を使って推論リクエストを送信します。

### 3.1 コンテナ内へのモデルのダウンロード

現在、Ollamaサーバーは起動していますが、中にはまだ一つもモデルが存在しません。`docker exec` コマンドを使うと、実行中のコンテナの内部で任意のコマンドを実行できます。これを利用して、OllamaコンテナにLLMをダウンロードさせましょう。

ここでは、比較的小さく扱いやすいモデルとして `llama3.2:8b` をダウンロードします。

```bash
docker exec -it ollama ollama pull llama3.2:8b
```

このコマンドは、`ollama` という名前のコンテナ内で、対話的（-it）に `ollama pull llama3.2:8b` を実行するようDockerに指示します。ダウンロードが完了したら、`ollama list` コマンドでモデルが正しくインストールされたかを確認できます。

```bash
docker exec ollama ollama list
```

`NAME` の列に `llama3.2:8b` が表示されるはずです。

### 3.2 curlによる推論リクエストの送信

モデルの準備ができたので、いよいよ推論リクエストを送信します。Ollamaは `/api/generate` というエンドポイントを提供しており、ここにHTTP POSTリクエストを送ることで推論を実行できます。

以下の `curl` コマンドをコピーして、ターミナルで実行してみてください。

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:8b",
  "prompt": "Explain the importance of Docker for MLOps in three key points.",
  "stream": false
}'
```

このコマンドは、先ほどダウンロードした `llama3.2:8b` モデルに対し、「MLOpsにおけるDockerの重要性を3つのキーポイントで説明せよ」というプロンプトを送信しています。しばらくすると、モデルが生成したテキストを含むJSONレスポンスが返ってきます。このリクエスト内容は、APIドキュメントに基づいています。

### 3.3 APIリクエストの解剖

OllamaのAPIは、単にプロンプトに応答を返すだけでなく、より高度なアプリケーションを構築するための豊富な機能を備えています。`/api/generate` エンドポイントの主要なリクエストパラメータを理解することは、その第一歩です。

**表3.1: /api/generate 主要リクエストパラメータ**

| パラメータ | 型 | 必須 | 説明 |
| :---- | :---- | :---- | :---- |
| model | string | はい | 使用するモデルの名前（例: `llama3.2:8b`）。事前にダウンロードまたは作成されている必要があります。 |
| prompt | string | はい | モデルへの入力テキスト。 |
| stream | boolean | いいえ | `false`の場合、完全なレスポンスが単一のJSONで返されます。`true`（デフォルト）の場合、テキストが生成されるにつれてJSONオブジェクトのストリームが返されます。リアルタイム性が求められるチャットボット等で不可欠です。 |
| format | string | いいえ | `"json"` に設定すると、出力が有効なJSON文字列であることが保証されます。プロンプト内でもJSONで出力するよう指示することが推奨されます。 |
| options | object | いいえ | `temperature`（創造性）、`seed`（再現性の確保）など、モデルのパラメータを上書きするためのキーと値のマップです。 |

Ollamaには、単発のタスク生成に適した `/api/generate` の他に、会話の履歴を維持できる `/api/chat` エンドポイントも用意されています。

### 3.4 APIレスポンスの理解

APIからのレスポンスもまた、構造化されたJSON形式です。`stream: false` の場合のレスポンスには、以下のような重要な情報が含まれています。

* `response`: モデルが生成した完全なテキスト。
* `done`: 生成が完了したことを示すブーリアン値 (`true`)。
* `context`: この会話のコンテキスト情報。次回の`generate`リクエストに含めることで、会話の記憶を維持できます。
* `total_duration`: リクエスト処理にかかった総時間（ナノ秒）。
* `eval_count`: レスポンス内のトークン数。
* `eval_duration`: レスポンス生成にかかった時間（ナノ秒）。

これらの情報から、`eval_count` を `eval_duration` で割ることで、トークン毎秒（tokens/sec）の生成速度を計算でき、パフォーマンスの指標とすることが可能です。

APIがstreamや詳細なパフォーマンスメトリクス、contextによる会話維持、`format: "json"`による構造化データ出力といった機能を提供していることは、Ollamaが単なる実験ツールではなく、洗練された本番アプリケーションへの組み込みを想定して設計されていることを示唆しています。

## 4. 再現性の核心：Dockerfileでイメージを構築する

セクション3では、`docker exec` を使って実行中のコンテナに「命令」を下し、状態を変化させました。この方法は手軽ですが、手順が手作業に依存するため、再現性に欠けます。

このセクションでは、LLMアプリケーションを真に再現可能でポータブルなものにするための核心的な技術、Dockerfile を用いたイメージ構築を学びます。これは、一回限りの命令的な操作から、誰でも同じ結果を得られる「宣言的」な設計へと移行することを意味します。

### 4.1 宣言的な設計図としてのDockerfile

Dockerfile は、コンテナイメージを構築するための手順を記述したテキストファイルです。ここに「最終的にどういう状態のイメージが欲しいか」を宣言的に記述することで、Dockerはその設計図通りにイメージを自動で構築してくれます。

### 4.2 テクニック：イメージへのモデルの焼き込み

ここからは、この章で最も重要なハンズオンです。特定のLLMモデルをイメージ自体に「焼き込む（bake-in）」ことで、モデルを含んだ自己完結型のAPIサーバーを作成します。

まず、新しいプロジェクトディレクトリを作成し、その中に `Dockerfile` という名前のファイルを作成してください。そして、以下の内容を記述します。

```dockerfile
# Step 1: 公式のOllamaベースイメージから開始する
# 再現性のために、特定のバージョンをピン留めすることがベストプラクティス
FROM ollama/ollama:latest

# Step 2: 目的のLLMモデルをイメージレイヤーに直接焼き込む
# このRUNコマンドは `docker build` プロセス中に実行される
# モデルはイメージ内の /root/.ollama に保存される
# 注意: このコマンドは数分かかる場合があり、イメージサイズが大幅に増加する
RUN ollama pull llama3.2:8b

# Step 3: Ollamaサーバーが待ち受けるポートを公開する
# これはイメージの利用者に、どのポートをマッピングすべきかを知らせるメタデータ
EXPOSE 11434

# Step 4: このイメージからコンテナを起動した際のデフォルトコマンドを定義する
# これによりOllama APIサーバーが起動し、リクエストを受け付ける準備が整う
CMD ["ollama", "serve"]
```

このDockerfileは、`ollama pull`コマンドをビルドプロセス中に実行することで、モデルファイルをイメージの一部として組み込むというアプローチを実装したものです。各命令の役割は以下の通りです。

* `FROM`: ビルドの土台となるベースイメージを指定します。
* `RUN`: イメージのビルド中に実行されるコマンドです。ここでモデルをダウンロードすることで、イメージレイヤーの一部として保存されます。
* `EXPOSE`: コンテナがリッスンするネットワークポートをドキュメント化します。
* `CMD`: このイメージからコンテナが起動されるときに、デフォルトで実行されるコマンドを指定します。

### 4.3 カスタムイメージのビルドと実行

Dockerfileが完成したら、`docker build` コマンドでカスタムイメージをビルドします。

```bash
docker build -t my-llm-app:1.0 .
```

`-t` フラグは、イメージに `my-llm-app` という名前と `1.0` というタグを付けます。末尾の `.` は、カレントディレクトリをビルドコンテキスト（Dockerfileや関連ファイルがある場所）として使用することを意味します。

ビルドが完了したら、新しいイメージからコンテナを実行します。

```bash
docker run -d -p 11434:11434 --name my-llm-server my-llm-app:1.0
```

ここで最も重要な点は、セクション2で使った `-v` フラグが**存在しない**ことです。もはやホストマシンからモデルをマウントする必要はありません。モデルはイメージの中に完全に内包されています。

### 4.4 検証：再現性の証明

これが、この章における「なるほど！」の瞬間です。コンテナを起動した直後に、以下のコマンドを実行してみてください。

```bash
docker exec my-llm-server ollama list
```

`llama3.2:8b` モデルが即座にリストに表示されるはずです。これは、モデルがビルド時にイメージに含まれていたことの動かぬ証拠です。

続けて、セクション3.2で使った `curl` リクエストを送信してみましょう。

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:8b",
  "prompt": "Explain the importance of Docker for MLOps in three key points.",
  "stream": false
}'
```

これも問題なく動作するはずです。

この `my-llm-app:1.0` イメージは、Docker Hubのようなコンテナリポジトリにプッシュすれば、チームの誰もが `docker run` コマンド一発で、全く同じモデルが入った、全く同じ動作をするサーバーを起動できます。これで「私の環境では動く」問題は完全に解決されました。

このモデルを焼き込むアプローチは、モデルを単なる「データ」から、アプリケーションの不可欠な「コンポーネント」へと昇格させます。イメージタグ `my-llm-app:1.0` は、Ollamaサーバーの特定のバージョンと `llama3.2:8b` モデルの特定の組み合わせを指すようになります。これにより、モデルのバージョン管理やロールバックが、`my-llm-app:0.9` のような古いイメージタグをデプロイするだけで可能になり、モデルのライフサイクルを標準的なソフトウェア開発のライフサイクルに統合できるのです。これは、成熟したMLOpsプラクティスの根幹をなす考え方です。

## 5. 発展的なコンセプトとベストプラクティス

自己完結したLLM APIサーバーを構築できるようになった今、より実践的なシナリオに対応するための高度な概念とベストプラクティスに目を向けましょう。

### 5.1 モデル管理戦略：焼き込み vs. ボリュームマウント

この章では、「モデルの焼き込み（Bake-in）」と「ボリュームマウント」という2つのアプローチを学びました。どちらか一方が絶対的に優れているわけではなく、状況に応じて適切な戦略を選択することが重要です。

**表5.1: モデル管理戦略の比較**

| 側面 | 「焼き込み」（Dockerfile RUN） | 「ボリュームマウント」（-v フラグ） |
| :---- | :---- | :---- |
| **イメージサイズ** | 巨大（ギガバイト単位） | 小さい（メガバイト単位） |
| **ビルド/プッシュ時間** | 遅い | 速い |
| **再現性** | 非常に高い（アトミックな単位） | 低い（外部ボリュームの状態に依存） |
| **可搬性** | 非常に高い（自己完結） | 低い（モデルの別途管理が必要） |
| **イテレーション速度** | 遅い（モデル変更に再ビルドが必要） | 速い（ボリュームに新モデルをpullするだけ） |
| **最適なユースケース** | ステージング/本番リリース、CI/CD成果物、一貫性の確保 | ローカル開発、迅速な実験、モデルが非常に大きいか頻繁に変更される場合 |

### 5.2 Modelfileによるモデルのカスタマイズ

Ollamaは、既存のモデルをカスタマイズするための `Modelfile` という仕組みを提供しています。これを使えば、単にモデルを利用するだけでなく、特定の振る舞いをするように調整できます。主な用途は以下の通りです。

* システムプロンプトの変更（例：「あなたは優秀なアシスタントです」）
* `temperature` などのデフォルトパラメータの設定
* 独自に用意したGGUF形式のモデルのインポート

例として、モデルの性格を海賊風に変える `Modelfile` を作成してみましょう。

```
# 既存のモデルから開始
FROM llama3.2:8b

# デフォルトパラメータを設定
PARAMETER temperature 0.7

# カスタムのシステムプロンプトを定義して、モデルの性格を変更
SYSTEM """You are a pirate chatbot. All your answers must be in the style of a pirate."""
```

この `Modelfile` を使って新しいモデル `my-pirate-model` を作成し、それをイメージに焼き込むには、Dockerfileを以下のように変更します。

```dockerfile
FROM ollama/ollama:latest

# Modelfileをビルドコンテキストにコピー
COPY ./Modelfile /app/Modelfile

# ollama create を使ってModelfileから新しいモデルを作成
RUN ollama create my-pirate-model -f /app/Modelfile

#... CMDやEXPOSEは同様...
CMD ["ollama", "serve"]
```

このDockerfileでビルドしたイメージを実行すると、`"model": "my-pirate-model"` を指定してAPIを呼び出すことで、海賊風の応答を得られるようになります。

### 5.3 プロダクションに向けて：マルチステージビルド

本番環境向けのイメージを構築する際は、イメージサイズを最小限に抑え、セキュリティを向上させることが重要です。そのための強力なテクニックが「マルチステージビルド」です。

これは、ビルドに必要なツールが含まれる「ビルダー」ステージと、実行に必要な最小限の成果物だけをコピーした、クリーンな「最終」ステージの2段階でイメージを構築する手法です。

Ollamaのイメージ自体はすでに最適化されていますが、例えばOllamaをバックエンドとして利用するPythonアプリケーション（FastAPIなど）を同梱する場合に、この原則は非常に有効です。以下に概念的なDockerfileを示します。

```dockerfile
# Stage 1: The Builder (Pythonアプリのビルド環境)
FROM python:3.9-slim as app_builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .

# Stage 2: The Final Image (実行環境)
FROM ollama/ollama:latest

# モデルをダウンロード
RUN ollama pull llama3.2:8b

# 最初のステージからビルド済みのPythonアプリケーションをコピー
COPY --from=app_builder /app /app

WORKDIR /app

# Ollamaとアプリの両方のポートを公開
EXPOSE 11434
EXPOSE 8000

# 実際には、OllamaサーバーとPythonアプリの両方を起動するスクリプトが必要
# CMD ["./start-services.sh"]
```

この例は、マルチステージビルドのベストプラクティスと、複数のサービスを組み合わせるマイクロサービス的な考え方を組み合わせた、より現実的な構成を示しています。

## 6. まとめと次のステップ

### 6.1 学習内容の振り返り

この章では、LLM APIサーバーのコンテナ化について、段階的に学びました。手作業の `ollama run` から始まり、ボリュームを利用したローカル開発向けの `docker run` を経て、最終的には Dockerfile を用いてモデルを内包した、完全に再現可能でポータブルなAPIサーバーイメージを構築しました。

そして、モデル管理における「焼き込み」と「ボリュームマウント」の戦略的なトレードオフを理解し、どちらのアプローチがどのような状況に適しているかを判断する基準を学びました。この知識は、単なるツールの使い方を超え、LLMアプリケーションのアーキテクチャを設計する上で不可欠なものです。

### 6.2 次のステップ

この章で作成したDockerイメージは、それ自体が完成品であると同時に、より大きなシステムを構築するための基本的な「ビルディングブロック」です。ここから先のステップとして、以下のような展開が考えられます。

* **Docker Compose:** 複数のコンテナを連携させて一つのアプリケーションとして管理するには、Docker Composeが次のステップとなります。例えば、今回作成したLLMサーバーコンテナと、Open WebUIのようなWebインターフェースコンテナ、あるいはチャット履歴を保存するデータベースコンテナを、一つの `docker-compose.yml` ファイルで定義し、一括で起動・停止できるようになります。より複雑なシステム構築には、[[declarative-rag-system-docker-compose]]が参考になります。
* **コンテナオーケストレーション:** 本番環境でのスケーリング、高可用性、ゼロダウンタイムでのデプロイメントを実現するには、Kubernetesのようなコンテナオーケストレーションプラットフォームにこのイメージをデプロイすることになります。作成したイメージは、これらの高度なプラットフォームで実行するための標準的な配布単位となります。

あなたは今、LLMを単なるローカルツールとして使う段階から、堅牢なソフトウェアコンポーネントとしてシステムに組み込むための、確かな第一歩を踏み出しました。
