---
tags:
  - kubernetes
  - docker
  - orchestration
  - beginner
  - infrastructure
  - minikube
---
# 第4章：Kubernetes入門：AIアプリケーションのスケーラブルな基盤

## 4.1 序論：単一サーバーから分散システムへ — なぜ「オーケストレーション」が必要なのか？

これまでの章、特に[[inference-acceleration-with-gpu|第3章]]では、AIアプリケーションをコンテナ化し、単一のサーバー上で効率的に実行するための技術を習得しました。[[introduction-to-docker|Docker]]を用いることで、開発環境と本番環境の差異をなくし、依存関係をパッケージ化してポータビリティを高めることができました。これは、AI開発のライフサイクルにおける大きな一歩です。しかし、プロトタイプから本格的なプロダクションサービスへと移行する段階で、私たちは新たな、そしてより複雑な課題に直面します。それは「スケール」の壁です。

あなたの開発したAIモデルが成功を収め、ユーザー数が10人から1万人、100万人へと急増するシナリオを想像してみてください。単一サーバーでは、増え続けるリクエストを処理しきれなくなり、レスポンスタイムは悪化し、最終的にはシステムダウンに至るでしょう。この問題に対する直感的な解決策は「サーバーを増やす」ことです。しかし、この単純な答えは、悪夢のような運用上の複雑さを引き起こします。

実際に、AIシステムをスケールさせる過程で、多くの企業が深刻な課題に直面しています。ある調査によれば、AIシステムのスケールアップ時に70%の企業が平均7ヶ月もの遅延を経験しており、その主な原因はリソース需要の増大、インフラの制約、そしてパフォーマンス問題です。AIワークロード、特に大規模言語モデル（LLM）は、計算リソースを大量に消費する「飢えた獣」に例えられます。リクエスト数が増えるにつれて、CPU、メモリ、そして高価なGPUの需要は指数関数的に増加します。

サーバーを2台、10台、100台と増やしていくと、次のような疑問が次々と湧き上がってきます。

*   **高可用性（High Availability）：** もし1台のサーバーがハードウェア障害やソフトウェアのバグで停止したら、どうやってサービスを継続させるのか？手動で別のサーバーにプロセスを再起動するのは、24時間365日稼働が求められるサービスでは非現実的です。
*   **負荷分散（Load Balancing）：** 殺到するリクエストを、複数のサーバーにどうやって均等に振り分けるのか？特定のサーバーに負荷が集中しないようにする仕組みが必要です。
*   **リソース管理（Resource Management）：** 複数のAIサービスが限られたCPUやGPUリソースを奪い合う状況をどう管理するのか？あるサービスがリソースを独占し、他のサービスが飢餓状態に陥ることを防がなければなりません。
*   **デプロイメントとロールバック（Deployment & Rollbacks）：** アプリケーションの新しいバージョンを、サービスを停止させることなく（ゼロダウンタイムで）全サーバーに展開するにはどうすればよいか？もし新しいバージョンに致命的なバグがあった場合、どうやって迅速に以前の安定したバージョンに戻すのか？
*   **監視とメンテナンス（Monitoring & Maintenance）：** 数十、数百のサーバーと、その上で動く無数のコンテナの状態をどうやって把握し、維持管理するのか？AIプロジェクトの失敗の70%は、パフォーマンスとメンテナンスの問題に起因するというデータもあります。

これらの課題は、手作業での管理が不可能であることを示しています。この運用上の「複雑さの危機」を解決するために生まれたのが、「コンテナオーケストレーション」という考え方です。コンテナオーケストレーションとは、分散環境におけるコンテナ化されたアプリケーションのライフサイクル（デプロイ、スケーリング、ネットワーキング、可用性など）を自動化する技術です。そして、この分野における事実上の世界標準（デファクトスタンダード）となっているのが、本章で学ぶ**Kubernetes**です。

Kubernetesの導入は、単なるツールの採用以上の、根本的なパラダイムシフトを意味します。それは、「個々のサーバーを管理する」という発想から、「アプリケーション全体の**あるべき状態（Desired State）**を宣言する」という発想への転換です。エンジニアはもはや、「サーバーAでこのコンテナを起動し、サーバーBに負荷を分散する」といった命令的な（Imperative）指示を出す必要はありません。代わりに、「このAPIアプリケーションのインスタンス（レプリカ）を5つ稼働させ、外部から特定のネットワークアドレスでアクセスできるようにせよ」という宣言的な（Declarative）設定ファイルを記述します。あとはKubernetesが、現在の状態を監視し、宣言された「あるべき状態」と異なる部分があれば、自動的にコンテナを起動・停止・移動させて差分を埋めてくれます。この抽象化により、エンジニアは低レベルなインフラ管理の煩雑さから解放され、アプリケーションのアーキテクチャや信頼性の向上といった、より価値の高いタスクに集中できるようになるのです。

さらに、大規模AIシステムのスケールを考える上で、計算リソースだけでなく、ネットワークもまた深刻なボトルネックとなり得ます。特に、クラスター内のサーバー間で行われる通信（「East-Westトラフィック」と呼ばれる）は、分散学習や複数のコンポーネントで構成される推論パイプラインにおいて爆発的に増加します。もしネットワークが混雑し、パケットの遅延や損失が発生すれば、たとえ高性能なGPUを持っていても、処理に必要なデータが届かずにアイドル状態となり、莫大なコストを無駄にすることになりかねません。この事実は、スケーラブルなAI基盤には、計算リソースのスケジューリングだけでなく、サービス間の発見（Service Discovery）や内部の負荷分散といった高度なネットワーキング機能が不可欠であることを示唆しています。Kubernetesは、まさにこのような課題を解決するために設計された、包括的なマイクロサービスプラットフォームなのです。

本章では、この強力なコンテナオーケストレーションシステムであるKubernetesの基本を学び、AIアプリケーションのためのスケーラブルな基盤を構築する第一歩を踏み出します。

## 4.2 AI/MLワークロードにおけるKubernetesの利点

Kubernetesがコンテナオーケストレーションの標準であることは述べましたが、なぜ特にリソース集約的で要求の厳しいAI/MLワークロードにとって、これほどまでに強力な選択肢となるのでしょうか。その理由は、Kubernetesが提供する機能群が、AI開発と運用のライフサイクルにおける特有の課題を的確に解決するためです。

*   **スケーラビリティと弾力性（Scalability and Elasticity）：** AI/MLアプリケーション、特に推論サービスのエンドポイントは、需要の変動が激しいことが特徴です。Kubernetesは、CPU使用率や受信リクエスト数などのリアルタイムメトリクスに基づいて、アプリケーションのインスタンス（Pod）の数を自動的に増減させる「Horizontal Pod Autoscaling」という機能を持っています。これにより、トラフィックの急増時にはシームレスにスケールアウトしてパフォーマンスを維持し、需要が低い時にはスケールインして余分なリソースを解放することで、コストを大幅に削減できます。
*   **リソースの最適化（特にGPU）：** AIワークロードのコストの大部分を占めるのが、GPUのような高価なハードウェアです。Kubernetesは、これらの特殊なリソースを効率的に管理・スケジューリングするための洗練されたメカニズムを提供します。タスクの要件に応じて適切なハードウェアリソースをコンテナに割り当て、アイドル状態のGPUを減らし、高価な資産の利用率を最大化します。これは、サーバーごとにリソースを静的に割り当てる従来の方法に比べて、劇的なコスト効率の向上をもたらします。
*   **高可用性と自己修復（High Availability and Self-Healing）：** 本番環境のサービスにとって、ダウンタイムは致命的です。Kubernetesは、回復力（レジリエンス）を前提に設計されています。もしコンテナがクラッシュしたり、サーバー（Node）全体が応答しなくなったりした場合でも、Kubernetesは自動的にその障害を検知し、失敗したコンテナを再起動したり、正常な別のNode上に新しいコンテナをスケジュールし直したりします。この「自己修復」能力により、人手を介さずともサービスの継続性が保たれ、システムの安定性が格段に向上します。
*   **ポータビリティとインフラ非依存（Portability and Infrastructure Independence）：** Kubernetesは、特定のクラウドプロバイダーや物理的な環境に縛られない、標準化されたプラットフォームを提供します。ローカルのKubernetesクラスタで動作するようにパッケージ化されたアプリケーションは、コードや設定を一切変更することなく、Amazon EKS, Google GKE, Azure AKSといった主要なマネージドクラウドサービスや、自社のオンプレミス環境にデプロイできます。これにより、ベンダーロックインを回避し、ビジネス要件に応じてインフラを柔軟に選択・変更する自由が得られます。
*   **AIエコシステムのための拡張性（Extensibility for the AI Ecosystem）：** Kubernetesは閉じたシステムではありません。「Operator」や「Custom Resource Definition (CRD)」といった仕組みを通じて、その機能を自由に拡張できます。この拡張性を活かし、Kubernetes上でのAI/MLワークフローを簡素化するための強力なツールエコシステムが形成されています。その代表例が**Kubeflow**で、分散学習、ハイパーパラメータチューニング、モデルサービングといった複雑なタスクを、Kubernetesのネイティブなリソースとして管理できるようにします。

これらの利点を深く掘り下げると、KubernetesがAIインフラにもたらす変革の本質が見えてきます。一つは、高価で専門的なハードウェアであるGPUの管理方法の変革です。従来、データサイエンティストは特定のGPU搭載サーバーを占有的に利用することが多く、これはGPUが使われていない時間も多い非効率なモデルでした。Kubernetesは、NVIDIA GPU Operatorのようなツールと連携することで、クラスター内のすべてのGPUを単一の、動的にスケジュール可能なリソースプールとして扱えるようにします。GPUを必要とするジョブ（Pod）を、空いている任意のGPU搭載Nodeに割り当て、リソース制限を適用し、さらには1つのGPUを複数のコンテナで共有する「タイムスライシング」のような高度な機能もサポートします。このGPUアクセスの民主化は、リソース利用率を劇的に向上させ、コストを削減し、より多くの実験を並行して実行することを可能にします。これは「マシンを予約する」モデルから「リソースを要求する」モデルへの転換であり、はるかに効率的でスケーラブルな運用を実現します。

もう一つの本質的な変革は、MLOpsにおける科学的厳密性と再現性の担保です。AI/ML開発は実験的なプロセスであり、結果の再現性は極めて重要です。モデルのコードだけでは不十分で、実行環境、ライブラリのバージョン、設定といったすべてが揃って初めて再現可能と言えます。Kubernetesでは、アプリケーションをコンテナとしてパッケージ化し、そのデプロイ方法のすべてをYAMLファイルに宣言的に記述します。このYAMLファイルをGitでバージョン管理する「**GitOps**」というプラクティスを組み合わせることで、いつでも、誰でも、全く同じ環境で学習ジョブや推論サービスを再構築することが保証されます。これは「私のマシンでは動いたのに」という典型的な問題を根絶するだけでなく、金融や医療といった規制の厳しい業界において、モデルがどのように学習・デプロイされたかの完全な監査証跡を提供します。このように、Kubernetesの採用は、MLOpsライフサイクル全体のエンジニアリング的な成熟度を飛躍的に高める技術的基盤となるのです。

## 4.3 開発環境の準備：MinikubeでローカルKubernetesクラスタを構築

理論的な背景を理解したところで、いよいよ実践に移ります。本章の後半で行うハンズオンラボを進めるためには、手元で動作するKubernetesクラスタが必要です。幸いなことに、自身のPC上に完全な機能を備えたシングルノードのKubernetesクラスタを簡単に構築できるツールが存在します。このセクションでは、そのための環境準備をステップバイステップで進めます。

### ローカルクラスタツールの選択

ローカル環境でKubernetesを動かすためのツールはいくつかありますが、特に広く使われているのが**Minikube**と**Kind**です。

*   **Minikube:** ローカルKubernetes開発ツールとして最も歴史があり、機能が豊富です。仮想マシン（VM）やDockerコンテナなど、様々な「ドライバ」をサポートしており、Kubernetes DashboardやIngressコントローラといった便利なアドオンも簡単に導入できるため、学習用途に非常に適しています。
*   **Kind (Kubernetes IN Docker):** より軽量で高速な代替ツールであり、その名の通りDockerコンテナをKubernetesの「ノード」として利用します。主にKubernetes自体のテストやCI/CDパイプラインでの利用を想定して設計されており、一時的なクラスタの作成・破棄を迅速に行えます。

本教科書では、**Minikube**をDockerドライバと共に使用することを推奨します。これは、パフォーマンスと機能のバランスが良く、ドキュメントも充実しており、アドオンシステムが学習の助けとなるためです。

### インストール手順

以下の手順に従って、開発環境をセットアップしてください。

#### ステップ1：前提条件の確認（Docker）

MinikubeをDockerドライバで動かすため、Docker DesktopまたはDocker Engineがインストールされている必要があります。まだインストールしていない場合は、公式サイトの指示に従ってインストールしてください。Windowsユーザーの場合は、Docker DesktopがLinuxコンテナを使用する設定になっていることを確認してください。

#### ステップ2：kubectlのインストール

kubectlは、Kubernetesクラスタと対話するためのコマンドラインインターフェース（CLI）です。Minikubeだけでなく、あらゆるKubernetesクラスタ（クラウド上のマネージドサービスなど）の操作に必須のツールです。

Kubernetesの公式ドキュメントに従って、お使いのオペレーティングシステム（macOS, Windows, Linux）用のkubectlをインストールしてください。

*   ([https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/))
*   ([https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/](https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/))
*   ([https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/))

#### ステップ3：Minikubeのインストール

次にMinikube本体をインストールします。これも公式サイトのスタートガイドに従うのが最も確実です。

**macOS (Homebrewを使用):**
```bash
brew install minikube
```

**Windows (PowerShellを使用):**
```powershell
New-Item -Path 'c:\' -Name 'minikube' -ItemType Directory -Force
Invoke-WebRequest -OutFile 'c:\minikube\minikube.exe' -Uri 'https://storage.googleapis.com/minikube/releases/latest/minikube-windows-amd64.exe' -UseBasicParsing
```
その後、c:\minikubeをシステムの環境変数Pathに追加します。

**Linux:**
```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
```

#### ステップ4：ローカルクラスタの起動

インストールが完了したら、ターミナルを開き、以下のコマンドを実行してDockerをドライバとして指定し、Minikubeクラスタを起動します。
```bash
minikube start --driver=docker
```
初回起動時は、Kubernetesコンポーネントのコンテナイメージのダウンロードなどが行われるため、数分かかることがあります。

#### ステップ5：インストールの検証

クラスタが正常に起動したことを確認しましょう。以下のコマンドを実行します。

**クラスタ情報の確認:**
```bash
kubectl cluster-info
```
このコマンドは、KubernetesのコントロールプレーンとCoreDNSのURLを表示します。エラーが出なければ、kubectlがクラスタと正常に通信できています。

**ノードの状態確認:**
```bash
kubectl get nodes
```
以下のような出力が表示され、minikubeという名前のノードがReady状態になっていれば成功です。
```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   5m    v1.28.3
```

これで、ハンズオンラボに進むための準備が整いました。

ここで注目すべきは、これから皆さんが行う「コードを書き、コンテナイメージをビルドし、マニフェストファイルを作成し、クラスタに適用する」という一連のローカルでの開発ループが、実は本番環境で運用される**GitOpsベースのCI/CDパイプラインの縮図**であるという点です。ローカルで作成するDockerfileはCIパイプラインのビルドステージに、YAMLマニフェストファイルはGitOpsで管理される設定リポジトリに、そして`kubectl apply`コマンドはArgo CDのようなCDツールが自動的に行うデプロイプロセスに相当します。つまり、このローカルでの開発サイクルを習得することは、単にいくつかのコマンドを覚えるだけでなく、クラウドネイティブな開発の基本的なワークフローを体得することに他なりません。ここで身につけるスキルは、後の章で学ぶより高度な自動化の世界へ直接繋がっています。

## 4.4 Kubernetesのアーキテクチャ：コアコンセプトを理解する

アプリケーションをデプロイする前に、Kubernetesがどのような「部品」で構成されているかを理解する必要があります。ここでは、ハンズオンラボで実際に使用する4つの最も基本的かつ重要なリソース（オブジェクト）、「Namespace」「Pod」「Deployment」「Service」について解説します。これらの概念は、Kubernetesを扱う上での共通言語となります。

| コンポーネント | アナロジー | 主要な目的 | 公式ドキュメント |
| :--- | :--- | :--- | :--- |
| **Namespace** | オフィスビルの特定のフロアや部署 | Kubernetesクラスタ内に作られる仮想的な区画。リソースをチームや環境（開発、本番など）ごとに分離・整理するために使用する。 | kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ |
| **Pod** | 1人または複数の住人（コンテナ）が同居するアパートの1室 | Kubernetesにおけるデプロイ可能な最小・最基本単位。密接に関連する1つ以上のコンテナを動かすためのラッパー。 | kubernetes.io/docs/concepts/workloads/pods/ |
| **Deployment** | アパートの管理人 | 同一で状態を持たない（ステートレスな）Pod群を管理する。スケーリング、アップデート、指定された数のレプリカの維持を担当する。 | kubernetes.io/docs/concepts/workloads/controllers/deployment/ |
| **Service** | 建物の住所とインターホンシステム | Pod群にアクセスするための安定したIPアドレスとDNS名を提供する。Podの短命性を隠蔽し、負荷分散を行う。 | kubernetes.io/docs/concepts/services-networking/service/ |

### Namespace（名前空間）

Namespaceは、一つの物理的なKubernetesクラスタを、複数の論理的な仮想クラスタに分割するための仕組みです。例えば、開発チーム用の`dev`、検証チーム用の`qa`、本番用の`prod`といったようにNamespaceを分けることで、各チームは他のチームのリソースを気にすることなく、独立して作業を進めることができます。リソース（PodやServiceなど）の名前は、同一Namespace内ではユニークでなければなりませんが、異なるNamespace間であれば同じ名前を使用できます。

クラスタ作成時には、いくつかのデフォルトNamespaceが用意されています。

*   `default`: Namespaceを明示的に指定しなかった場合に、リソースが作成される場所。
*   `kube-system`: Kubernetesシステム自体が使用するコンポーネント（CoreDNSなど）が配置される場所。
*   `kube-public`: 認証されていないユーザーを含め、全ユーザーが読み取り可能なリソースを置くための場所。

ハンズオンでは、私たちのアプリケーション用に専用のNamespaceを作成し、リソースを整理する良い習慣を身につけます。

### Pod（ポッド）

Podは、Kubernetesで作成および管理できる、コンピューティングの最小単位です。Podは、1つ以上のコンテナのグループであり、これらのコンテナはストレージとネットワークリソースを共有し、常に同じサーバー（Node）上で稼働することが保証されます。

最もシンプルなケースでは、Podは1つのコンテナを内包し、両者は1対1の関係になります。これは、私たちのAPIサーバーのような単機能のアプリケーションで一般的な構成です。より高度なケースでは、メインのアプリケーションコンテナと、ログを収集・転送する「サイドカー」コンテナを同じPodに入れるといった使い方もされます。重要なのは、Kubernetesはコンテナを直接管理するのではなく、Podという単位で管理するということです。スケーリングやスケジューリングはPod単位で行われます。

### Deployment（デプロイメント）

Podは単体で作成することもできますが、それらは障害時に自動で復旧しません。Podが稼働していたNodeがダウンすれば、Podも失われます。プロダクション環境では、アプリケーションの可用性を維持するための仕組みが必要です。そこで登場するのがDeploymentです。

Deploymentは、同一のPodの集合（レプリカセット）を管理するための、より高レベルなリソースです。Deploymentの主な役割は、YAMLファイルで定義された「あるべき状態」、例えば「このPodのレプリカを3つ常に稼働させておく」という宣言を維持することです。もし何らかの理由でPodの1つが停止すれば、Deploymentのコントローラーがそれを検知し、即座に新しいPodを立ち上げてレプリカ数を3つに戻します。

また、Deploymentはアプリケーションのアップデート（ローリングアップデート）や、問題があった際のロールバックも宣言的に管理できるため、私たちのAPIサーバーのようなステートレスなアプリケーションを実行する際の標準的な方法となっています。

### Service（サービス）

Deploymentによって、アプリケーションのPodは障害時にも自動で復旧し、スケーリングも可能になりました。しかし、Podは本質的に短命（Ephemeral）です。再起動したり、別のNodeに移動したりすると、その都度新しいIPアドレスが割り当てられます。これでは、他のアプリケーションや外部のユーザーが、どのIPアドレスにアクセスすればよいのか分かりません。

この問題を解決するのがServiceです。Serviceは、論理的に定義されたPodの集合に対して、単一の安定したアクセスポイント（IPアドレスとDNS名）を提供する抽象化レイヤーです。Serviceは、指定されたラベル（例：`app: my-api`）を持つPod群を継続的に監視し、それらのPodへのトラフィックを自動的に振り分ける内部的なロードバランサーとして機能します。

Serviceにはいくつかのタイプがありますが、ハンズオンでは`NodePort`タイプを使用します。これは、クラスタ内の各Nodeの静的なポートでServiceを公開するもので、Minikubeのようなローカル環境でクラスタ外部からアプリケーションに簡単にアクセスするのに便利です。

これらの4つのコンポーネントの関係をまとめると、「**Namespace**という区画の中で、**Deployment**が管理する複数の**Pod**（アプリケーション本体）に対して、**Service**が安定した窓口を提供する」という構図になります。この構造を理解することが、Kubernetesを使いこなすための鍵となります。

## 4.5 ハンズオンラボ：Flask APIサーバーのデプロイとスケーリング

いよいよ、これまでに学んだ概念を実践に移します。このハンズオンラボでは、シンプルな機械学習APIを模したFlaskアプリケーションを題材に、以下の手順をゼロから実行します。

1.  Flaskアプリケーションのコード準備
2.  Dockerfileによるコンテナイメージの作成
3.  Kubernetesマニフェスト（DeploymentとService）の作成
4.  Minikubeクラスタへのデプロイと動作確認
5.  アプリケーションのスケーリング

このラボを完了することで、あなたはKubernetes上でステートレスなAIアプリケーションを運用するための基本的なスキルセットを習得できます。

### 4.5.1 サンプルアプリケーション（Python Flask）

まず、デプロイするアプリケーションを用意します。これは、リクエストを受け取ってダミーの予測結果を返す、非常にシンプルなAPIサーバーです。

プロジェクト用のディレクトリを作成し、その中に`app.py`という名前で以下のファイルを作成してください。

**app.py:**
```python
from flask import Flask, jsonify, request

app = Flask(__name__)

@app.route('/')
def home():
    return jsonify({"message": "Welcome to the AI Model API!"})

@app.route('/predict', methods=['POST'])
def predict():
    # 実際には、ここで受け取ったデータを使ってMLモデルで推論を行う
    # この例ではダミーのレスポンスを返す
    data = request.get_json(force=True)
    text_input = data.get('text', 'no input')

    # ダミーの予測ロジック
    prediction = {
        "input_text": text_input,
        "predicted_class": "Positive",
        "confidence": 0.98
    }

    return jsonify(prediction)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

このAPIには、ウェルカムメッセージを返す`/`エンドポイントと、POSTリクエストを受け取って予測結果を模したJSONを返す`/predict`エンドポイントがあります。

次に、アプリケーションの依存関係を定義します。同じディレクトリに`requirements.txt`というファイルを作成し、以下の内容を記述します。

**requirements.txt:**
```
Flask
```

### 4.5.2 Dockerfileによるコンテナ化

次に、このFlaskアプリケーションをコンテナ化するためのDockerfileを作成します。ここでは、最終的なイメージサイズを小さく、よりセキュアにするためのベストプラクティスである**マルチステージビルド**を採用します。

プロジェクトのルートディレクトリに`Dockerfile`という名前でファイルを作成し、以下の内容を記述してください。

**Dockerfile:**
```dockerfile
# --- ステージ1: ビルダー ---
# 依存関係をインストールするためのステージ
FROM python:3.9-slim as builder

WORKDIR /app

# 依存関係定義ファイルをコピー
COPY requirements.txt .

# 依存関係をインストール
RUN pip install --no-cache-dir -r requirements.txt

# --- ステージ2: ファイナル ---
# 実際にアプリケーションを実行する軽量なステージ
FROM python:3.9-slim

WORKDIR /app

# ビルダーステージからインストール済みのライブラリのみをコピー
COPY --from=builder /usr/local/lib/python3.9/site-packages /usr/local/lib/python3.9/site-packages

# アプリケーションコードをコピー
COPY app.py .

# コンテナがリッスンするポートを公開
EXPOSE 5000

# コンテナ起動時に実行するコマンド
CMD ["python", "app.py"]
```

各命令の意味は以下の通りです。

*   `FROM python:3.9-slim as builder`: `builder`という名前のビルドステージを開始し、軽量なPythonイメージをベースにします。
*   `WORKDIR /app`: コンテナ内の作業ディレクトリを`/app`に設定します。
*   `COPY requirements.txt .`: `requirements.txt`をホストからコンテナにコピーします。
*   `RUN pip install...`: pipを使って依存関係をインストールします。
*   `FROM python:3.9-slim`: ここから最終ステージが始まります。再度軽量イメージをベースにします。
*   `COPY --from=builder...`: ステージ1（`builder`）から、インストール済みのライブラリ群を最終ステージにコピーします。これにより、ビルドのためだけの中間ファイルが最終イメージに含まれるのを防ぎます。
*   `COPY app.py .`: アプリケーションのソースコードをコピーします。
*   `EXPOSE 5000`: このコンテナがポート5000をリッスンすることをDockerに伝えます。
*   `CMD ["python", "app.py"]`: コンテナが起動したときに実行されるデフォルトのコマンドです。

ターミナルで以下のコマンドを実行し、Dockerイメージをビルドします。
```bash
docker build -t flask-api:v1 .
```
これで、`flask-api:v1`という名前とタグのついたコンテナイメージが作成されました。MinikubeはホストマシンのDockerデーモンを共有するため、このイメージは特別な操作なしにMinikubeクラスタから利用可能です。

### 4.5.3 Kubernetesマニフェストの作成

次に、Kubernetesに「アプリケーションをどのように実行してほしいか」を伝えるための設定ファイル（マニフェスト）をYAML形式で作成します。今回は`deployment.yaml`と`service.yaml`の2つのファイルを作成します。

**deployment.yaml:**
このファイルは、アプリケーションのPodをどのようにデプロイし、管理するかを定義します。
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-api-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flask-api
  template:
    metadata:
      labels:
        app: flask-api
    spec:
      containers:
      - name: flask-api-container
        image: flask-api:v1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000
```

*   `kind: Deployment`: これがDeploymentリソースであることを示します。
*   `spec.replicas: 1`: Podのインスタンスを1つ作成するよう指示します。
*   `spec.selector.matchLabels`: このDeploymentが管理対象とするPodを識別するためのラベルを指定します。ここでは`app: flask-api`というラベルを持つPodを探します。
*   `spec.template`: Deploymentが作成するPodの設計図（テンプレート）です。
*   `template.metadata.labels`: ここでPodに`app: flask-api`というラベルを付けています。これにより、上記のselectorがこのPodを見つけられるようになります。この**ラベルとセレクターの組み合わせが、Kubernetesでリソース同士を連携させるための基本**です。
*   `template.spec.containers`: Pod内で実行するコンテナのリストです。
*   `image: flask-api:v1`: 先ほどビルドしたDockerイメージを指定します。
*   `imagePullPolicy: IfNotPresent`: イメージがローカルに存在する場合は、リモートのレジストリからプルしないように指示します。Minikubeでローカルイメージを使う際には必須の設定です。
*   `containerPort: 5000`: コンテナがリッスンしているポート番号です。

**service.yaml:**
このファイルは、デプロイされたPod群に外部からアクセスするための安定した窓口を定義します。
```yaml
apiVersion: v1
kind: Service
metadata:
  name: flask-api-service
spec:
  type: NodePort
  selector:
    app: flask-api
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000
```

*   `kind: Service`: これがServiceリソースであることを示します。
*   `spec.type: NodePort`: このServiceを、クラスタの各Nodeの特定のポートを通じて外部に公開するよう指示します。
*   `spec.selector`: このServiceがトラフィックを転送すべきPodを選択するためのラベルです。Deploymentが作成するPodのラベル（`app: flask-api`）と一致させている点に注目してください。
*   `spec.ports`: ポートのマッピングを定義します。
    *   `port: 80`: Service自体のポート番号です（クラスタ内部でのアクセス用）。
    *   `targetPort: 5000`: トラフィックの転送先であるPod内のコンテナがリッスンしているポートです。
    *   `nodePort`: `type: NodePort`の場合、外部からアクセスするためのポートがNode上に開かれます。ここでは指定していないため、Kubernetesが30000-32767の範囲から自動的にポートを割り当てます。

### 4.5.4 デプロイと動作確認

マニフェストファイルが準備できたので、`kubectl`を使ってクラスタに適用（apply）します。
```bash
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
```
出力として`deployment.apps/flask-api-deployment created`と`service/flask-api-service created`が表示されれば成功です。

次に、リソースが正しく作成されたかを確認します。

**Deploymentの状態を確認:**
```bash
kubectl get deployments
```
`READY`カラムが`1/1`になっていれば、Podが正常に起動しています。

**Podの状態を確認:**
```bash
kubectl get pods
```
`flask-api-deployment-`で始まる名前のPodが`Running`状態であることを確認します。

**Serviceの状態を確認:**
```bash
kubectl get services
```
`flask-api-service`の`TYPE`が`NodePort`になっており、`PORT(S)`カラムにポートマッピング（例: `80:30007/TCP`）が表示されていることを確認します。この`30007`が、外部からアクセスするためのnodePortです。

最後に、アプリケーションにアクセスしてみましょう。Minikubeには、ServiceへのURLを簡単に取得し、ブラウザで開くための便利なコマンドがあります。
```bash
minikube service flask-api-service
```
このコマンドを実行すると、自動的にブラウザが開き、「Welcome to the AI Model API!」というメッセージが表示されるはずです。

`curl`を使って`/predict`エンドポイントも試してみましょう。まず、`minikube service flask-api-service --url`でURLを取得し、そのURLに対してPOSTリクエストを送ります。
```bash
# URLを取得 (出力は環境によって異なります)
URL=$(minikube service flask-api-service --url)

# POSTリクエストを送信
curl -X POST -H "Content-Type: application/json" -d '{"text": "This is a great movie!"}' $URL/predict
```
以下のような予測結果のJSONが返ってくれば、デプロイは完全に成功です。
```json
{
  "confidence": 0.98,
  "input_text": "This is a great movie!",
  "predicted_class": "Positive"
}
```

### 4.5.5 アプリケーションのスケーリング

これで、あなたのAIアプリケーションはKubernetes上で稼働しました。最後に、この章の核心的な目標である「スケーリング」を実践します。トラフィックの増加に対応するため、アプリケーションのインスタンス（レプリカ）を増やす操作を見ていきましょう。

#### 方法1：命令的なスケーリング（kubectl scale）

最も手軽にスケールするには`kubectl scale`コマンドを使います。以下のコマンドを実行して、レプリカ数を3に増やしてみましょう。
```bash
kubectl scale deployment flask-api-deployment --replicas=3
```
コマンド実行後、すぐにPodの状態を確認します。
```bash
kubectl get pods
```
`flask-api-deployment-`で始まるPodが3つ`Running`状態になっているのが分かります。Kubernetesは指示通り、新たに追加で2つのPodを起動しました。

この方法は非常に直感的で素早いですが、変更がYAMLファイルに反映されないため、「今、本番環境で何レプリカ動いているか」の唯一の信頼できる情報源（Single Source of Truth）が失われてしまいます。テストや一時的な対応には便利ですが、本番環境での継続的な管理には推奨されません。

#### 方法2：宣言的なスケーリング（YAMLファイルの更新）

GitOpsの思想に沿った、よりプロフェッショナルで推奨される方法が、マニフェストファイルを更新することです。

`deployment.yaml`ファイルを開き、`replicas:`の値を`5`に変更してください。
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-api-deployment
spec:
  replicas: 5  # ここを 1 から 5 に変更
  selector:
    matchLabels:
      app: flask-api
  #... 以下省略...
```
ファイルを保存したら、再度`kubectl apply`コマンドを実行します。
```bash
kubectl apply -f deployment.yaml
```
KubernetesのDeploymentコントローラーは、クラスタの現在の状態（レプリカ数3）と、新たに適用されたマニフェストの「あるべき状態」（レプリカ数5）を比較します。そして、その差分である2つのPodを自動的に追加で作成し、状態を一致させます。

再度Podの状態を確認すると、今度は5つのPodが稼働しているはずです。
```bash
kubectl get pods
```
この宣言的なアプローチこそが、Kubernetesの真価を発揮する使い方です。YAMLファイルをGitリポジトリで管理することで、インフラストラクチャの構成そのものがコードとしてバージョン管理され、誰が、いつ、どのような変更を行ったかの完全な履歴が残ります。インフラの変更はすべてGitへのコミットとプルリクエストを通じて行われ、`kubectl`を直接実行することは原則としてありません。この**Infrastructure as Code (IaC)**の実践が、再現性、監査性、そしてチームでの協業を可能にする、現代的なシステム管理の根幹をなすのです。

## 4.6 まとめと次のステップ

本章では、単一サーバーでの運用から、複数サーバーへのスケールアウトという大きな壁を乗り越えるための強力なソリューションとして、コンテナオーケストレーションシステムKubernetesを学びました。

まず、AIアプリケーションをスケールさせる際に直面する、可用性、負荷分散、リソース管理といった運用上の複雑さを明らかにし、それらを解決する手段としてKubernetesが必要とされる理由を理解しました。次に、AI/MLワークロード特有の要求（スケーラビリティ、GPU管理、再現性など）に対してKubernetesが提供する具体的な利点を探りました。

実践パートでは、Minikubeを使ってローカルに開発環境を構築し、Kubernetesの基本的な構成要素である**Pod**、**Deployment**、**Service**、**Namespace**の役割を学びました。そして、ハンズオンラボを通じて、コンテナ化したFlask APIサーバーをKubernetesクラスタ上にデプロイし、NodePort Serviceを使って外部に公開し、最後に`kubectl scale`コマンドとYAMLファイルの更新という2つの方法でアプリケーションをスケールさせる具体的な手順を習得しました。

この章で身につけた知識とスキルは、Kubernetesの世界への入り口に過ぎませんが、これからの学習の確固たる土台となります。あなたが今立っている場所から、プロダクションレベルのAIインフラを構築するための道筋は、次のように続いています。

*   **ステートフルなワークロード（データベース、モデルストア）：** 今回扱ったAPIサーバーは、どのインスタンスがリクエストを処理しても問題ない「ステートレス」なアプリケーションでした。しかし、データベースやLLMのモデルファイルのように、状態（データ）を永続的に保持する必要があるコンポーネントはどうでしょうか？Kubernetesは、Podのライフサイクルから独立したストレージを管理するための**PersistentVolume (PV)** と **PersistentVolumeClaim (PVC)** という仕組みを提供しています。第6章では、これらのリソースを使ってステートフルなアプリケーションを管理する方法を学びます。
*   **GPUアクセラレーションによる学習と推論：** 私たちのAPIはCPU上で動作しましたが、本格的なAIワークロードにはGPUが不可欠です。クラスター全体に散らばる高価なGPUリソースを効率的に管理し、必要なジョブに割り当てるのは複雑なタスクです。第7章では、**NVIDIA GPU Operator**を導入し、GPUをKubernetesのネイティブなリソースとして扱えるようにする方法を探求します。これにより、効率的な分散学習や大規模な推論サービスの基盤が整います。
*   **オブザーバビリティ（監視とロギング）：** アプリケーションを稼働させ、スケールできるようになった今、次なる課題は「そのアプリケーションが正常に動いているかどうやって知るか」です。リソース使用率、レイテンシ、エラーレートなどを追跡し、問題が発生した際に即座に検知する仕組みがなければ、安定したサービスは提供できません。第8章では、メトリクス収集のための**Prometheus**と、それを可視化するための強力なダッシュボードツール**Grafana**を組み合わせ、包括的な監視（オブザーバビリティ）スタックを構築します。
*   **自動化とGitOps：** ハンズオンでは、YAMLファイルを手動で編集し、`kubectl apply`で適用しました。実際のプロダクション環境では、このプロセスは完全に自動化されます。Kubernetesの宣言的な性質は、**GitOps**と呼ばれるプラクティスのための完璧な基盤です。これは、Gitリポジトリをインフラの唯一の信頼できる情報源とし、Gitへの変更を自動的にクラスタに同期させる手法です。第9章では、**Argo CD**のようなツールを使い、Gitリポジトリへのプッシュをトリガーとしてアプリケーションが自動でデプロイ・更新される、完全なCI/CDパイプラインを構築します。

Kubernetesへの旅は、単に新しいツールを学ぶことではありません。それは、回復力があり、スケーラブルで、自動化されたシステムを構築・管理するための新しい哲学を採り入れることです。本章で学んだ概念は、現代のMLOpsとクラウドネイティブなAIインフラを支える、不可欠なビルディングブロックなのです。
